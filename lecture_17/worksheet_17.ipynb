{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 17\n",
    "\n",
    "Name: Woohyeon Her <br>\n",
    "UID: U88838753\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Recommender Systems\n",
    "\n",
    "### Recommender Systems\n",
    "\n",
    "In the example in class of recommending movies to users we used the movie rating as a measure of similarity between users and movies and thus the predicted rating for a user is a proxy for how highly a movie should be recommended. So the higher the predicted rating for a user, the higher a recommendation it would be.\n",
    "\n",
    "a) Consider a streaming platform that only has \"like\" or \"dislike\" (no 1-5 rating). Describe how you would build a recommender system in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection: Gather user interactions with movies in a binary form (like or dislike). <br>\n",
    "Preprocessing: Clean the data and handle missing values or imbalances in likes and dislikes. <br>\n",
    "Model Selection: Choose a model based on the available computational resources, data characteristics, and desired recommendation quality. <br>\n",
    "Training and Validation: Train the model using historical data and validate using a holdout set or cross-validation. <br>\n",
    "Deployment: Integrate the recommender into the platform's backend to serve real-time recommendations. <br>\n",
    "Feedback Loop: Continuously update the model with new user data to refine and improve recommendations. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Describe 3 challenges of building a recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Building a recommender system presents several challenges, including data sparsity and scalability, where the vast amount of uninteracted items creates sparse matrices that are computationally intensive to process. The cold start problem complicates making accurate recommendations for new users or items without historical interaction data, potentially affecting user satisfaction. Lastly, ensuring diversity and serendipity in recommendations is crucial to avoid repetitiveness and enhance user engagement by introducing unexpected choices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Why is SVD not an option for collaborative filtering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SVD (Singular Value Decomposition) is not typically ideal for collaborative filtering due to several key challenges. Firstly, SVD cannot directly handle sparse matrices, which are common in collaborative filtering where users only interact with a small subset of items, leaving many matrix entries as unknown. Secondly, SVD's computational cost is high, making it less practical for large datasets typical in recommender systems, and it lacks scalability for real-time recommendations. Finally, SVD assumes linear relationships and is sensitive to noise, which may not accurately capture the complex and non-linear interactions in user-item data. Alternative techniques like ALS, stochastic gradient descent-based matrix factorization, and SVD++ are often preferred as they are designed to address these specific limitations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Use the code below to train a recommender system on a dataset of amazon movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Note: requires py3.10\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "# Note: requires py3.10\n",
    "import findspark\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "findspark.init()\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.memory\",\"28g\")\n",
    "conf.set(\"spark.driver.memory\", \"28g\")\n",
    "conf.set(\"spark.driver.cores\", \"8\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "init_df = pd.read_csv(\"./train.csv\").dropna()\n",
    "init_df['UserId_fact'] = init_df['UserId'].astype('category').cat.codes\n",
    "init_df['ProductId_fact'] = init_df['ProductId'].astype('category').cat.codes\n",
    "\n",
    "# Split training set into training and testing set\n",
    "X_train_processed, X_test_processed, Y_train, Y_test = train_test_split(\n",
    "        init_df.drop(['Score'], axis=1),\n",
    "        init_df['Score'],\n",
    "        test_size=1/4.0,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "X_train_processed['Score'] = Y_train\n",
    "df = spark.createDataFrame(X_train_processed[['UserId_fact', 'ProductId_fact', 'Score']])\n",
    "als = ALS(\n",
    "    userCol=\"UserId_fact\",\n",
    "    itemCol=\"ProductId_fact\",\n",
    "    ratingCol=\"Score\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    rank=100\n",
    ")\n",
    "# param_grid = ParamGridBuilder().addGrid(\n",
    "        # als.rank, [10, 50]).addGrid(\n",
    "        # als.regParam, [.1]).addGrid(\n",
    "        # # als.maxIter, [10]).build()\n",
    "# evaluator = RegressionEvaluator(\n",
    "        # metricName=\"rmse\",\n",
    "        # labelCol=\"Score\", \n",
    "        # # predictionCol=\"prediction\")\n",
    "# cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3, parallelism = 6)\n",
    "# cv_fit = cv.fit(df)\n",
    "# rec_sys = cv_fit.bestModel\n",
    "\n",
    "rec_sys = als.fit(df)\n",
    "# rec_sys.save('rec_sys.obj') # so we don't have to re-train it\n",
    "rec = rec_sys.transform(spark.createDataFrame(X_test_processed[['UserId_fact', 'ProductId_fact']])).toPandas()\n",
    "X_test_processed['Score'] = rec['prediction'].values.reshape(-1, 1)\n",
    "\n",
    "print(\"Kaggle RMSE = \", mean_squared_error(X_test_processed['Score'], Y_test, squared=False))\n",
    "\n",
    "cm = confusion_matrix(Y_test, X_test_processed['Score'], normalize='true')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import findspark\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Initialize Spark\n",
    "findspark.init()\n",
    "conf = SparkConf().setAppName(\"MovieRecommender\")\n",
    "conf.set(\"spark.executor.memory\", \"28g\")\n",
    "conf.set(\"spark.driver.memory\", \"28g\")\n",
    "conf.set(\"spark.driver.cores\", \"8\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Load and preprocess data\n",
    "init_df = pd.read_csv(\"./train.csv\").dropna()\n",
    "init_df['UserId_fact'] = init_df['UserId'].astype('category').cat.codes\n",
    "init_df['ProductId_fact'] = init_df['ProductId'].astype('category').cat.codes\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_processed, X_test_processed, Y_train, Y_test = train_test_split(\n",
    "    init_df[['UserId_fact', 'ProductId_fact']], \n",
    "    init_df['Score'],\n",
    "    test_size=0.25,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Prepare Spark DataFrame from the training data\n",
    "X_train_processed['Score'] = Y_train\n",
    "train_df = spark.createDataFrame(X_train_processed)\n",
    "\n",
    "# Define ALS model\n",
    "als = ALS(\n",
    "    userCol=\"UserId_fact\",\n",
    "    itemCol=\"ProductId_fact\",\n",
    "    ratingCol=\"Score\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    rank=100\n",
    ")\n",
    "\n",
    "# Build parameter grid for model tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 50, 100]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(als.maxIter, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluator as RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"Score\"\n",
    ")\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=6\n",
    ")\n",
    "\n",
    "# Fit ALS model\n",
    "cv_model = cv.fit(train_df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Prepare test data\n",
    "test_df = spark.createDataFrame(X_test_processed)\n",
    "X_test_processed['Score'] = Y_test  # to compare with predictions\n",
    "\n",
    "# Predict on test data\n",
    "predictions = best_model.transform(test_df).toPandas()\n",
    "X_test_processed['Predicted_Score'] = predictions['prediction']\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = mean_squared_error(X_test_processed['Score'], X_test_processed['Predicted_Score'], squared=False)\n",
    "print(\"Kaggle RMSE =\", rmse)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(X_test_processed['Score'], X_test_processed['Predicted_Score'], normalize='true')\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\")\n",
    "plt.title('Confusion Matrix of the Classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Stop Spark session\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
